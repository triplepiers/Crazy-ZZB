{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 机器人自动走迷宫\n","\n","<br>\n","<hr>"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["# 1. 实验介绍  "]},{"cell_type":"markdown","metadata":{},"source":["## 1.1 实验内容  \n","在本实验中，要求分别使用基础搜索算法和 Deep QLearning 算法，完成机器人自动走迷宫。\n"," \n","<img src=\"https://imgbed.momodel.cn/20200914145238.png\" width=\"40%\"/>\n","\n","如上图所示，左上角的红色椭圆既是起点也是机器人的初始位置，右下角的绿色方块是出口。          \n","游戏规则为：从起点开始，通过错综复杂的迷宫，到达目标点(出口)。\n","        \n","+ 在任一位置可执行动作包括：向上走 `'u'`、向右走 `'r'`、向下走 `'d'`、向左走 `'l'`。\n","\n","+ 执行不同的动作后，根据不同的情况会获得不同的奖励，具体而言，有以下几种情况。\n","    - 撞墙\n","    - 走到出口\n","    - 其余情况\n","    \n","    \n","+ 需要您分别实现**基于基础搜索算法**和 **Deep QLearning 算法**的机器人，使机器人自动走到迷宫的出口。"]},{"cell_type":"markdown","metadata":{},"source":["## 1.2 实验要求 \n","+ 使用 Python 语言。\n","+ 使用基础搜索算法完成机器人走迷宫。\n","+ 使用 Deep QLearning 算法完成机器人走迷宫。\n","+ 算法部分需要自己实现，不能使用现成的包、工具或者接口。\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1.3 实验环境\n","可以使用 Python 实现基础算法的实现， 使用 Keras、PyTorch等框架实现 Deep QLearning 算法。"]},{"cell_type":"markdown","metadata":{},"source":["## 1.4 注意事项\n","+ Python 与 Python Package 的使用方式，可在右侧 `API文档` 中查阅。\n","+ 当右上角的『Python 3』长时间指示为运行中的时候，造成代码无法执行时，可以重新启动 Kernel 解决（左上角『Kernel』-『Restart Kernel』）。"]},{"cell_type":"markdown","metadata":{},"source":["## 1.5 参考资料\n","+  强化学习入门MDP：https://zhuanlan.zhihu.com/p/25498081\n","+ QLearning 示例：http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n","+ QLearning 知乎解释：https://www.zhihu.com/question/26408259\n","+ DeepQLearning 论文：https://files.momodel.cn/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pdf\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["# 2. 实验内容\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["\n","## 2.1 Maze 类介绍"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1.1 创建迷宫\n","通过迷宫类 Maze 可以随机创建一个迷宫。\n","\n","1. 使用  Maze(maze_size=size)  来随机生成一个 size * size 大小的迷宫。\n","2. 使用 print() 函数可以输出迷宫的 size 以及画出迷宫图\n","3. 红色的圆是机器人初始位置\n","4. 绿色的方块是迷宫的出口位置"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"select":true},"outputs":[],"source":["# 导入相关包\n","import os\n","import random\n","import numpy as np\n","from Maze import Maze\n","from Runner import Runner\n","from QRobot import QRobot\n","from ReplayDataSet import ReplayDataSet\n","from torch_py.MinDQNRobot import MinDQNRobot as TorchRobot # PyTorch版本\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","\n","\"\"\" 创建迷宫并展示 \"\"\"\n","maze = Maze(maze_size=10) # 随机生成 N*N 迷宫\n","print(maze)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1.2 重要的成员方法\n","在迷宫中已经初始化一个机器人，你要编写的算法实现在给定条件下控制机器人移动至目标点。\n","\n","Maze 类中重要的成员方法如下：\n","\n","1. sense_robot() ：获取机器人在迷宫中目前的位置。\n","\n","> return：机器人在迷宫中目前的位置。\n","\n","2. move_robot(direction) ：根据输入方向移动默认机器人，若方向不合法则返回错误信息。\n","\n","> direction：移动方向, 如:\"u\", 合法值为： ['u', 'r', 'd', 'l']\n","\n","> return：执行动作的奖励值\n","\n","3. can_move_actions(position)：获取当前机器人可以移动的方向\n","\n","> position：迷宫中任一处的坐标点 \n","\n","> return：该点可执行的动作，如：['u','r','d']\n","\n","4. is_hit_wall(self, location, direction)：判断该移动方向是否撞墙\n","\n","> location, direction：当前位置和要移动的方向，如(0,0) , \"u\"\n","\n","> return：True(撞墙) / False(不撞墙)\n","\n","5. draw_maze()：画出当前的迷宫\n"]},{"cell_type":"markdown","metadata":{},"source":["**随机移动机器人，并记录下获得的奖励，展示出机器人最后的位置。**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import random\n","\n","rewards = [] # 记录每走一步的奖励值\n","actions = [] # 记录每走一步的移动方向\n","\n","# 循环、随机移动机器人10次，记录下奖励\n","for i in range(10):\n","    valid_actions = maze.can_move_actions(maze.sense_robot())\n","    action = random.choice(valid_actions)\n","    rewards.append(maze.move_robot(action))\n","    actions.append(action)\n","\n","print(\"the history of rewards:\", rewards)\n","print(\"the actions\", actions)\n","\n","# 输出机器人最后的位置\n","print(\"the end position of robot:\", maze.sense_robot())\n","\n","# 打印迷宫，观察机器人位置\n","print(maze)\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["## 2.2 基础搜索算法介绍（广度优先搜索算法）\n","\n","对于迷宫游戏，常见的三种的搜索算法有广度优先搜索、深度优先搜索和最佳优先搜索（A*)。\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["在下面的代码示例中，将实现广度优先搜索算法；主要通过建立一颗搜索树并进行层次遍历实现。\n","+ 每个节点表示为以 `Class SearchTree` 实例化的对象，类属性有：**当前节点位置、到达当前节点的动作、当前节点的父节点、当前节点的子节点**；\n","+ `valid_actions():` 用以获取机器人可以行走的位置（即不能穿墙）；\n","+ `expand():` 对于未拓展的子节点进行拓展；\n","+ `backpropagation():` 回溯搜索路径。"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2.1 算法具体步骤\n","\n","首先以机器人起始位置建立根节点，并入队；接下来不断重复以下步骤直到判定条件:\n","\n","1. 将队首节点的位置标记已访问；判断队首是否为目标位置(出口)， **是** 则终止循环并记录回溯路径\n","2. 判断队首节点是否为叶子节点，**是** 则拓展该叶子节点\n","3. 如果队首节点有子节点，则将每个子节点插到队尾\n","4. 将队首节点出队"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2.2 编程实现广度优先搜索算法"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","# 机器人移动方向\n","move_map = {\n","    'u': (-1, 0), # up\n","    'r': (0, +1), # right\n","    'd': (+1, 0), # down\n","    'l': (0, -1), # left\n","}\n","\n","\n","# 迷宫路径搜索树\n","class SearchTree(object):\n","\n","\n","    def __init__(self, loc=(), action='', parent=None):\n","        \"\"\"\n","        初始化搜索树节点对象\n","        :param loc: 新节点的机器人所处位置\n","        :param action: 新节点的对应的移动方向\n","        :param parent: 新节点的父辈节点\n","        \"\"\"\n","\n","        self.loc = loc  # 当前节点位置\n","        self.to_this_action = action  # 到达当前节点的动作\n","        self.parent = parent  # 当前节点的父节点\n","        self.children = []  # 当前节点的子节点\n","\n","    def add_child(self, child):\n","        \"\"\"\n","        添加子节点\n","        :param child:待添加的子节点\n","        \"\"\"\n","        self.children.append(child)\n","\n","    def is_leaf(self):\n","        \"\"\"\n","        判断当前节点是否是叶子节点\n","        \"\"\"\n","        return len(self.children) == 0\n","\n","\n","def expand(maze, is_visit_m, node):\n","    \"\"\"\n","    拓展叶子节点，即为当前的叶子节点添加执行合法动作后到达的子节点\n","    :param maze: 迷宫对象\n","    :param is_visit_m: 记录迷宫每个位置是否访问的矩阵\n","    :param node: 待拓展的叶子节点\n","    \"\"\"\n","    can_move = maze.can_move_actions(node.loc)\n","    for a in can_move:\n","        new_loc = tuple(node.loc[i] + move_map[a][i] for i in range(2))\n","        if not is_visit_m[new_loc]:\n","            child = SearchTree(loc=new_loc, action=a, parent=node)\n","            node.add_child(child)\n","\n","\n","def back_propagation(node):\n","    \"\"\"\n","    回溯并记录节点路径\n","    :param node: 待回溯节点\n","    :return: 回溯路径\n","    \"\"\"\n","    path = []\n","    while node.parent is not None:\n","        path.insert(0, node.to_this_action)\n","        node = node.parent\n","    return path\n","\n","\n","def breadth_first_search(maze):\n","    \"\"\"\n","    对迷宫进行广度优先搜索\n","    :param maze: 待搜索的maze对象\n","    \"\"\"\n","    start = maze.sense_robot()\n","    root = SearchTree(loc=start)\n","    queue = [root]  # 节点队列，用于层次遍历\n","    h, w, _ = maze.maze_data.shape\n","    is_visit_m = np.zeros((h, w), dtype=np.int32)  # 标记迷宫的各个位置是否被访问过\n","    path = []  # 记录路径\n","    while True:\n","        current_node = queue[0]\n","        is_visit_m[current_node.loc] = 1  # 标记当前节点位置已访问\n","\n","        if current_node.loc == maze.destination:  # 到达目标点\n","            path = back_propagation(current_node)\n","            break\n","\n","        if current_node.is_leaf():\n","            expand(maze, is_visit_m, current_node)\n","\n","        # 入队\n","        for child in current_node.children:\n","            queue.append(child)\n","\n","        # 出队\n","        queue.pop(0)\n","\n","    return path\n"]},{"cell_type":"markdown","metadata":{},"source":["**测试广度优先搜索算法**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["maze = Maze(maze_size=10)\n","height, width, _ = maze.maze_data.shape\n","\n","path_1 = breadth_first_search(maze)\n","print(\"搜索出的路径：\", path_1)\n","\n","for action in path_1:\n","    maze.move_robot(action)\n","\n","if maze.sense_robot() == maze.destination:\n","    print(\"恭喜你，到达了目标点\")\n","\n","print(maze)\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["## 2.3 题目一: 实现基础搜索算法（40分）\n","* 题目要求： 任选深度优先搜索算法、最佳优先搜索 A* 算法其中一种实现机器人走迷宫\n","\n","* 输入：迷宫\n","\n","* 输出：到达目标点的路径\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["### 2.3.1 编写您的基础搜索算法"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"select":true},"outputs":[],"source":["def my_search(maze):\n","    \"\"\"\n","    任选深度优先搜索算法、最佳优先搜索（A*)算法实现其中一种\n","    :param maze: 迷宫对象\n","    :return :到达目标点的路径 如：[\"u\",\"u\",\"r\",...]\n","    \"\"\"\n","\n","    path = []\n","\n","    # -----------------请实现你的算法代码--------------------------------------\n","    N = maze.maze_size-1\n","    cur_pos = maze.sense_robot()\n","\n","    def opposite(direct):\n","        if direct == 'u':\n","            return 'd'\n","        elif direct == 'd':\n","            return 'u'\n","        elif direct == 'l':\n","            return 'r'\n","        elif direct == 'r':\n","            return 'l'\n","\n","    def DFS(cur_pos):\n","        i, j = cur_pos\n","        # go out\n","        if cur_pos == (N, N):\n","            return True\n","        else:\n","            options = maze.can_move_actions(cur_pos)\n","\n","            # no going BACK\n","            if len(path) > 0:\n","                options.remove(opposite(path[-1])) \n","            \n","            # no where to go\n","            if len(options) == 0:\n","                path.pop()\n","                return False\n","            else:\n","                for direction in options:\n","                    path.append(direction)\n","                    if direction == 'u':\n","                        if DFS((i-1, j)):\n","                            return True\n","                    elif direction == 'd':\n","                        if DFS((i+1, j)):\n","                            return True\n","                    elif direction == 'l':\n","                        if DFS((i, j-1)):\n","                            return True\n","                    elif direction == 'r':\n","                        if DFS((i, j+1)):\n","                            return True\n","                # no where to go\n","                path.pop()\n","                return False\n","\n","    DFS(cur_pos)\n","    # -----------------------------------------------------------------------\n","    return path\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3.2 测试您编写的基础搜索算法"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["maze = Maze(maze_size=10) # 从文件生成迷宫\n","\n","path_2 = my_search(maze)\n","print(\"搜索出的路径：\", path_2)\n","\n","for action in path_2:\n","    maze.move_robot(action)\n","\n","if maze.sense_robot() == maze.destination:\n","    print(\"恭喜你，到达了目标点\")\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["## 2.4 强化学习算法介绍\n","\n","强化学习作为机器学习算法的一种，其模式也是让智能体在“训练”中学到“经验”，以实现给定的任务。    \n","但不同于监督学习与非监督学习，在强化学习的框架中，我们更侧重通过智能体与环境的**交互**来学习。   \n","通常在监督学习和非监督学习任务中，智能体往往需要通过给定的训练集，辅之以既定的训练目标（如最小化损失函数），通过给定的学习算法来实现这一目标。    \n","然而在强化学习中，智能体则是通过其与环境交互得到的奖励进行学习。     \n","这个环境可以是虚拟的（如虚拟的迷宫），也可以是真实的（自动驾驶汽车在真实道路上收集数据）。\n","\n","\n","在强化学习中有五个核心组成部分，它们分别是：**环境（Environment）**、**智能体（Agent）**、**状态（State）**、**动作（Action）**和**奖励（Reward）**。\n","\n","在某一时间节点 $t$：\n","    \n","- 智能体在从环境中感知其所处的状态 $s_t$\n","- 智能体根据某些准则选择动作 $a_t$\n","- 环境根据智能体选择的动作，向智能体反馈奖励 $r_{t+1}$\n","\n","通过合理的学习算法，智能体将在这样的问题设置下，成功学到一个在状态 $s_t$ 选择动作 $a_t$ 的策略 $\\pi (s_t) = a_t$。\n","\n","<img src=\"https://imgbed.momodel.cn/20200914153419.png\" width=400px/>\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["\n","## 2.5 QLearning 算法\n","\n","Q-Learning 是一个值迭代（Value Iteration）算法。    \n","与策略迭代（Policy Iteration）算法不同，值迭代算法会计算每个”状态“或是”状态-动作“的值（Value）或是效用（Utility），然后在执行动作的时候，会设法最大化这个值。    \n","因此，对每个状态值的准确估计，是值迭代算法的核心。    \n","通常会考虑**最大化动作的长期奖励**，即不仅考虑当前动作带来的奖励，还会考虑动作长远的奖励。\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5.1 Q 值的计算与迭代\n","\n","Q-learning 算法将状态（state）和动作（action）构建成一张 Q_table 表来存储 Q 值，Q 表的行代表状态（state），列代表动作（action）：\n","\n","<img src=\"https://imgbed.momodel.cn/20200914161241.png\" width=400px/>\n","\n","在 Q-Learning 算法中，将这个长期奖励记为 Q 值，其中会考虑每个 ”状态-动作“ 的 Q 值，具体而言，它的计算公式为：\n","\n","$$\n","Q(s_{t},a) = R_{t+1} + \\gamma \\times\\max_a Q(a,s_{t+1})\n","$$\n","\n","也就是对于当前的“状态-动作” $(s_{t},a)$，考虑执行动作 $a$ 后环境奖励 $R_{t+1}$，以及执行动作 $a$ 到达 $s_{t+1}$后，执行任意动作能够获得的最大的Q值 $\\max_a Q(a,s_{t+1})$，$\\gamma$ 为折扣因子。\n","\n","计算得到新的 Q 值之后，一般会使用更为保守地更新 Q 表的方法，即引入松弛变量 $alpha$ ，按如下的公式进行更新，使得 Q 表的迭代变化更为平缓。\n","\n","$$\n","Q(s_{t},a) = (1-\\alpha) \\times Q(s_{t},a) + \\alpha \\times(R_{t+1} + \\gamma \\times\\max_a Q(a,s_{t+1}))\n","$$"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["### 2.5.2 机器人动作的选择\n","\n","在强化学习中，**探索-利用** 问题是非常重要的问题。    \n","具体来说，根据上面的定义，会尽可能地让机器人在每次选择最优的决策，来最大化长期奖励。    \n","但是这样做有如下的弊端：    \n","1. 在初步的学习中，Q 值是不准确的，如果在这个时候都按照 Q 值来选择，那么会造成错误。\n","2. 学习一段时间后，机器人的路线会相对固定，则机器人无法对环境进行有效的探索。\n","\n","因此需要一种办法，来解决如上的问题，增加机器人的探索。   \n","通常会使用 **epsilon-greedy** 算法：\n","1. 在机器人选择动作的时候，以一部分的概率随机选择动作，以一部分的概率按照最优的 Q 值选择动作。\n","2. 同时，这个选择随机动作的概率应当随着训练的过程逐步减小。\n","\n","<img src=\"http://imgbed.momodel.cn/20200602153554.png\" width=400>\n","<img src=\"http://imgbed.momodel.cn/20200601144827.png\" width=400>"]},{"cell_type":"markdown","metadata":{},"source":["### 2.5.3  Q-Learning 算法的学习过程\n","<img src=\"http://imgbed.momodel.cn/20200601170657.png\" width=900>"]},{"cell_type":"markdown","metadata":{},"source":["###  2.5.4 Robot 类\n","\n","在本作业中提供了 QRobot 类，其中实现了 Q 表迭代和机器人动作的选择策略，可通过 `from QRobot import QRobot` 导入使用。\n","\n","**QRobot 类的核心成员方法**\n","\n","1. sense_state()：获取当前机器人所处位置\n","\n","> return：机器人所处的位置坐标，如： (0, 0)\n","\n","2. current_state_valid_actions()：获取当前机器人可以合法移动的动作\n","\n","> return：由当前合法动作组成的列表，如： ['u','r']\n","\n","3. train_update()：以**训练状态**，根据 QLearning 算法策略执行动作\n","\n","> return：当前选择的动作，以及执行当前动作获得的回报, 如： 'u', -1\n","\n","4. test_update()：以**测试状态**，根据 QLearning 算法策略执行动作\n","\n","> return：当前选择的动作，以及执行当前动作获得的回报, 如：'u', -1\n","\n","5. reset()\n","\n","> return：重置机器人在迷宫中的位置"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from QRobot import QRobot\n","from Maze import Maze\n","\n","maze = Maze(maze_size=5) # 随机生成迷宫\n","\n","robot = QRobot(maze) # 记得将 maze 变量修改为你创建迷宫的变量名\n","\n","action, reward = robot.train_update() # QLearning 算法一次Q值迭代和动作选择\n","\n","print(\"the choosed action: \", action)\n","print(\"the returned reward: \", action)\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":true},"source":["### 2.5.5 Runner 类\n","\n","QRobot 类实现了 QLearning 算法的 Q 值迭代和动作选择策略。在机器人自动走迷宫的训练过程中，需要不断的使用 QLearning 算法来迭代更新 Q 值表，以达到一个“最优”的状态，因此封装好了一个类 Runner 用于机器人的训练和可视化。可通过 `from Runner import Runner` 导入使用。\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**Runner 类的核心成员方法：**\n","\n","1. run_training(training_epoch, training_per_epoch=150): 训练机器人，不断更新 Q 表，并讲训练结果保存在成员变量 train_robot_record 中\n","\n","> training_epoch, training_per_epoch: 总共的训练次数、每次训练机器人最多移动的步数\n","\n","2. run_testing()：测试机器人能否走出迷宫\n","\n","3. generate_gif(filename)：将训练结果输出到指定的 gif 图片中\n","\n","> filename：合法的文件路径,文件名需以 `.gif` 为后缀\n","\n","4. plot_results()：以图表展示训练过程中的指标：Success Times、Accumulated Rewards、Runing Times per Epoch\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","**设定训练参数、训练、查看结果**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from QRobot import QRobot\n","from Maze import Maze\n","from Runner import Runner\n","\n","\"\"\"  Qlearning 算法相关参数： \"\"\"\n","\n","epoch = 10  # 训练轮数\n","epsilon0 = 0.5  # 初始探索概率\n","alpha = 0.5  # 公式中的 ⍺\n","gamma = 0.9  # 公式中的 γ\n","maze_size = 5  # 迷宫size\n","\n","\"\"\" 使用 QLearning 算法训练过程 \"\"\"\n","\n","g = Maze(maze_size=maze_size)\n","r = QRobot(g, alpha=alpha, epsilon0=epsilon0, gamma=gamma)\n","\n","runner = Runner(r)\n","runner.run_training(epoch, training_per_epoch=int(maze_size * maze_size * 1.5))\n","\n","# 生成训练过程的gif图, 建议下载到本地查看；也可以注释该行代码，加快运行速度。\n","runner.generate_gif(filename=\"results/size5.gif\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["runner.plot_results() # 输出训练结果，可根据该结果对您的机器人进行分析。\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["## 2.6 题目二: 实现 Deep QLearning 算法（60分）"]},{"cell_type":"markdown","metadata":{},"source":["### 2.6.1 DQN 算法介绍\n","强化学习是一个反复迭代的过程，每一次迭代要解决两个问题：给定一个策略求值函数，和根据值函数来更新策略。而 DQN 算法使用神经网络来近似值函数。([DQN 论文地址](https://files.momodel.cn/Playing%20Atari%20with%20Deep%20Reinforcement%20Learning.pdf))\n","\n","+ **DQN 算法流程**\n","\n","<img src=\"https://imgbed.momodel.cn/20200918101051.png\" width=\"60%\"/>\n","\n","+ **DQN 算法框架图**\n","\n","<img src=\"https://imgbed.momodel.cn/20200918101137.png\" width=\"60%\"/>\n"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false},"source":["### 2.6.2 完成 DQN 算法"]},{"cell_type":"markdown","metadata":{},"source":["**ReplayDataSet 类的核心成员方法**\n","\n","+ add(self, state, action_index, reward, next_state, is_terminal) 添加一条训练数据\n","\n","> state: 当前机器人位置\n","\n","> action_index: 选择执行动作的索引\n","\n","> reward： 执行动作获得的回报\n","\n","> next_state：执行动作后机器人的位置\n","\n","> is_terminal：机器人是否到达了终止节点（到达终点或者撞墙）\n","\n","+ random_sample(self, batch_size)：从数据集中随机抽取固定batch_size的数据\n","\n","> batch_size: 整数，不允许超过数据集中数据的个数\n","\n","+ **build_full_view(self, maze)：开启金手指，获取全图视野**\n","\n","> maze: 以 Maze 类实例化的对象"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"ReplayDataSet 类的使用\"\"\"\n","\n","from ReplayDataSet import ReplayDataSet\n","\n","test_memory = ReplayDataSet(max_size=1e3) # 初始化并设定最大容量\n","actions = ['u', 'r', 'd', 'l']\n","test_memory.add((0,1), actions.index(\"r\"), -10, (0,1), 1)  # 添加一条数据（state, action_index, reward, next_state）\n","print(test_memory.random_sample(1)) # 从中随机抽取一条（因为只有一条数据）\n"]},{"cell_type":"markdown","metadata":{},"source":["#### （1）实现简单的 DQNRobot\n","\n","作业中提供了简单的 DQNRobot 实现，其中依靠简单的两层全连接神经网络决策动作\n","\n","<div align=left>\n","<center><img src=\"https://imgbed.momodel.cn/20201029220521.png\" width=\"241px\"/>\n","</div>\n","\n","+ **该神经网络的输入：机器人当前的位置坐标，输出：执行四个动作（up、right、down、left）的评估分数**"]},{"cell_type":"markdown","metadata":{},"source":["该部分我们支持 PyTorch 版本和 Keras 版本，大家可以选择自己擅长的深度学习框架！！！ 我们已经实现简单的 DQNRobot 部分，大家可以完善该部分代码！！！"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch_py.MinDQNRobot import MinDQNRobot as TorchRobot # PyTorch版本\n","from keras_py.MinDQNRobot import MinDQNRobot as KerasRobot # Keras版本\n","\n","import matplotlib.pyplot as plt\n","from Maze import Maze\n","from Runner import Runner\n","import os\n","\n","os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # 允许重复载入lib文件\n","\n","maze = Maze(maze_size=5)\n","\n","\n","\"\"\"选择keras版本或者torch版本的机器人, MinRobot是尽量选择reward值最小的动作，对象初始化过程中修改了maze的reward参数\"\"\"\n","# robot = KerasRobot(maze=maze)\n","robot = TorchRobot(maze=maze)\n","\n","print(robot.maze.reward) # 输出最小值选择策略的reward值\n","\n","\"\"\"开启金手指，获取全图视野\"\"\"\n","robot.memory.build_full_view(maze=maze) #\n","\n","\"\"\"training by runner\"\"\"\n","runner = Runner(robot=robot)\n","runner.run_training(training_epoch=10, training_per_epoch=75)\n","\n","\"\"\"Test Robot\"\"\"\n","robot.reset()\n","for _ in range(25):\n","    a, r = robot.test_update()\n","    print(\"action:\", a, \"reward:\", r)\n","    if r == maze.reward[\"destination\"]:\n","        print(\"success\")\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["#### （2）实现你自己的 DQNRobot\n","\n"," + **题目要求:** 编程实现 DQN 算法在机器人自动走迷宫中的应用\n"," + **输入:** 由 Maze 类实例化的对象 maze\n"," + **要求不可更改的成员方法：**train_update()、test_update() **注：不能修改该方法的输入输出及方法名称，测试评分会调用这两个方法**。\n"," + **补充1:**若要自定义的参数变量，在 \\_\\_init\\_\\_() 中以 `self.xxx = xxx` 创建即可\n"," + **补充2:**实现你自己的DQNRobot时，要求继承 QRobot 类，QRobot 类包含了某些固定的方法如reset(重置机器人位置),sense_state(获取机器人当前位置).."]},{"cell_type":"code","execution_count":25,"metadata":{"deletable":false,"select":true},"outputs":[],"source":["import random\n","\n","import numpy as np\n","\n","import torch\n","from torch import  optim\n","import torch.nn.functional as F \n","\n","from QRobot import QRobot\n","from torch_py.QNetwork import QNetwork\n","from ReplayDataSet import ReplayDataSet\n","\n","# additional: 修改 GQNetwork 中 NN 的隐藏层配置\n","\n","class Robot(QRobot):\n","\n","    def __init__(self, maze):\n","        \"\"\"\n","        初始化 Robot 类\n","        :param maze:迷宫对象\n","        \"\"\"\n","        super(Robot, self).__init__(maze)\n","\n","        self.step = 1\n","        if torch.cuda.is_available():\n","            self.device = torch.device(\"cuda:0\")\n","        else:\n","            self.device = torch.device(\"cpu\")\n","\n","        m_size = maze.maze_size\n","        # update reward (Minimize)\n","        maze.set_reward({\n","            \"hit_wall\": 10.,\n","            \"destination\": -5. * m_size**2,\n","            \"default\": 1.,\n","        })\n","        self.maze = maze\n","        self.maze_size = m_size\n","\n","        # defaults\n","        self.updateInterval = m_size**2 - 1\n","\n","        # init memo\n","        self.memory = ReplayDataSet(max_size=max(\n","            1e4, m_size**2 * 3\n","        ))\n","        self.memory.build_full_view(maze)\n","\n","        # init NNs\n","        self.batch_size         = len(self.memory.Experience)\n","        self.init_learning_rate = 0.001\n","        target_nn, eval_nn = None, None\n","        self._build_nn()\n","        \n","        return\n","\n","    def _build_nn(self):\n","        seed = 0\n","        random.seed(seed)\n","\n","        nn_config = {\n","            'state_size':  2,\n","            'action_size': 4,\n","            'seed':        seed,\n","            'mine':        True\n","        }\n","\n","        # init NN\n","        self.eval_nn   = QNetwork(**nn_config).to(self.device)\n","        self.target_nn = QNetwork(**nn_config).to(self.device)\n","\n","        # init optimizer(Adam)\n","        self.optimizer = optim.Adam(\n","            self.eval_nn.parameters(),\n","            lr=self.init_learning_rate\n","        )\n","\n","        return\n","    \n","    def _update_target_nn(self):\n","        self.target_nn.load_state_dict(self.eval_nn.state_dict())\n","        return\n","\n","    def _select_action(self, cur_state):\n","        state = torch.from_numpy(np.array(cur_state)).float().to(self.device)\n","\n","        if random.random() < self.epsilon:\n","            action = random.choice(self.valid_action)\n","        else:\n","            self.eval_nn.eval()\n","            with torch.no_grad():\n","                q_next = self.eval_nn(cur_state).cpu().data.numpy()\n","            self.eval_nn.train()\n","        \n","        return self.valid_action[np.argmin(q_next).item()]\n","\n","    def _get_reward(self, action):\n","        return self.maze.move_robot(action)\n","\n","    def _get_action_idx(self, action):\n","        return self.valid_action.index(action)\n","    \n","    def _choose_action(self, state):\n","        state = torch.from_numpy(\n","            np.array(self.sense_state(), dtype=np.int16)\n","        ).float().to(self.device)\n","\n","        if random.random() < self.epsilon:\n","            return random.choice(self.valid_action)\n","        else:\n","            self.eval_nn.eval()\n","            with torch.no_grad():\n","                q_next = self.eval_nn(state).cpu().data.numpy() \n","            self.eval_nn.train()\n","            return self.valid_action[np.argmin(q_next).item()]\n","\n","    def learn(self, batch_size):\n","        if len(self.memory.Experience) < batch_size:\n","            return\n","\n","        state, action_idx, reward, next_state, is_terminal = self.memory.random_sample(batch_size)\n","        \n","        state       = torch.from_numpy(state).float().to(self.device)\n","        action_idx  = torch.from_numpy(action_idx).long().to(self.device)\n","        reward      = torch.from_numpy(reward).float().to(self.device)\n","        next_state  = torch.from_numpy(next_state).float().to(self.device)\n","        is_terminal = torch.from_numpy(is_terminal).int().to(self.device)\n","\n","        self.eval_nn.train()\n","        self.target_nn.eval()\n","\n","        q_next = self.target_nn(next_state).detach().min(1)[0].unsqueeze(1)\n","        q_cur  = reward + self.gamma * q_next * (\n","            torch.ones_like(is_terminal) - is_terminal\n","        )\n","\n","        self.optimizer.zero_grad()\n","        q_pred = self.eval_nn(state).gather(dim=1, index=action_idx)\n","\n","        loss = F.mse_loss(q_pred, q_cur)\n","        loss_val = loss.item()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        self._update_target_nn()\n","\n","        return loss_val\n","\n","    def train_update(self):\n","        \"\"\"\n","        以训练状态选择动作并更新Deep Q network的相关参数\n","        :return :action, reward 如：\"u\", -1\n","        \"\"\"\n","\n","        # -----------------请实现你的算法代码--------------------------------------\n","        state  = self.sense_state()\n","        action = self._choose_action(state)\n","        reward = self._get_reward(action)\n","\n","        if self.step % self.updateInterval == 0:\n","            self.step = 1\n","            self.learn(self.batch_size)\n","\n","        self.step += 1\n","        # -----------------------------------------------------------------------\n","\n","        return action, reward\n","\n","    def test_update(self):\n","        \"\"\"\n","        以测试状态选择动作并更新Deep Q network的相关参数\n","        :return : action, reward 如：\"u\", -1\n","        \"\"\"\n","        # -----------------请实现你的算法代码--------------------------------------\n","        state = torch.from_numpy(\n","            np.array(self.sense_state(), dtype=np.int16)\n","        ).float().to(self.device)\n","        \n","        self.eval_nn.eval()\n","        with torch.no_grad():\n","            q_vals = self.eval_nn(state).cpu().data.numpy()\n","\n","        action = self.valid_action[np.argmin(q_vals).item()]\n","        reward = self._get_reward(action)\n","        # -----------------------------------------------------------------------\n","        return action, reward\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import time\n","\n","def train_my_robot(robot): # 使用时需要注视掉 train_update() 中自动 learn() 的部分\n","    loss_list  = []\n","    batch_size = len(robot.memory) \n","    m_size     = robot.maze.maze_size\n","    dst_reward = robot.maze.reward['destination']\n","\n","    start = time.time()\n","    while True:\n","        loss = robot.learn(batch_size)\n","        loss_list.append(loss)\n","        robot.reset()\n","        for _ in range(m_size ** 2 - 1):\n","            a, r = robot.test_update()\n","            if r == dst_reward:\n","                print('Training time: {:.2f} s'.format(time.time() - start))\n","                print(f'Need at least {len(loss_list)} epochs')\n","                return loss_list\n","\n","def test_my_robot(robot):\n","    robot.reset() # clear\n","\n","    m_size     = robot.maze.maze_size\n","    dst_reward = robot.maze.reward['destination']\n","    for step in range(m_size**2 - 1):\n","        a, r = robot.test_update()\n","        print(f'@Step {step+1}, action: {a}, reward: {r}')\n","        if r == dst_reward:\n","            print('Reach destionation @ TEST')\n","            break\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["#### （3）测试您的 DQN 算法\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from QRobot import QRobot\n","from Maze import Maze\n","from Runner import Runner\n","\n","\"\"\"  Deep Qlearning 算法相关参数： \"\"\"\n","\n","maze_size = 5  # 迷宫size\n","epoch = 20000  # 训练轮数\n","training_per_epoch = int(maze_size**2 - 1)\n","\n","\"\"\" 使用 DQN 算法训练 \"\"\"\n","\n","g = Maze(maze_size=maze_size)\n","print(g)\n","r = Robot(g)\n","runner = Runner(r)\n","\n","# train\n","runner.run_training(epoch, training_per_epoch) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test\n","r.reset()\n","for step in range(maze_size**2):\n","    a, re = r.test_update()\n","    print(\"Step [\", step+1, \"] action:\", a, \"reward:\", re)\n","    if re == g.reward[\"destination\"]:\n","        print(\"success\")\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["### 2.6.3 作业测试与提交"]},{"cell_type":"markdown","metadata":{},"source":["- 经过 `2.3` 与 `2.6` 分别测试使用基础算法、DQN算法实现机器人走出迷宫！\n","- 测试完成之后，点击左侧 `提交作业` 的标签中，把整个 Notebook 目标 cell 转化为 main.py 文件进行`系统测试`。\n","- 平台测试时请记得勾选 main.py 文件需要依赖的其它文件等。\n","- 通过测试就可以**提交作业**。\n","-  提交作业时请记得提交勾选 **『程序报告.docx』**或者 **『程序报告.pdf』**。"]},{"cell_type":"markdown","metadata":{},"source":["作业评分说明：\n","1. 满分100分，基础算法通过迷宫得 40 分， DQN 算法通过初级、中级、高级迷宫分别得 20 分。"]},{"cell_type":"markdown","metadata":{},"source":["**最后，祝愿您不仅能从中收获到满满的知识，而且收获到一个满意分数！**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
